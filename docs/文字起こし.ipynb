{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXhx7DeRKXMq"
      },
      "source": [
        "# 電算倶楽部 2022/12/10 WisperTest\n",
        "\n",
        "## See also\n",
        "https://dev.classmethod.jp/articles/whisper-how-to/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZa7InjuHR2D",
        "outputId": "49996acc-a144-4d72-984c-bd3419d6276d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (23.0)\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to c:\\users\\shimp\\appdata\\local\\temp\\pip-req-build-8ec3wel5\n",
            "  Resolved https://github.com/openai/whisper.git to commit 7858aa9c08d98f75575035ecd6481f462d66ca27\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: numpy in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from openai-whisper==20230124) (1.23.5)\n",
            "Requirement already satisfied: torch in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from openai-whisper==20230124) (1.13.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from openai-whisper==20230124) (4.64.1)\n",
            "Requirement already satisfied: more-itertools in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from openai-whisper==20230124) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.19.0 in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from openai-whisper==20230124) (4.25.1)\n",
            "Requirement already satisfied: ffmpeg-python==0.2.0 in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from openai-whisper==20230124) (0.2.0)\n",
            "Requirement already satisfied: future in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from ffmpeg-python==0.2.0->openai-whisper==20230124) (0.18.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers>=4.19.0->openai-whisper==20230124) (0.13.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers>=4.19.0->openai-whisper==20230124) (3.8.2)\n",
            "Requirement already satisfied: requests in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers>=4.19.0->openai-whisper==20230124) (2.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers>=4.19.0->openai-whisper==20230124) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers>=4.19.0->openai-whisper==20230124) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers>=4.19.0->openai-whisper==20230124) (22.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers>=4.19.0->openai-whisper==20230124) (0.11.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tqdm->openai-whisper==20230124) (0.4.6)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch->openai-whisper==20230124) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (1.26.12)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\shimp\\AppData\\Local\\Temp\\pip-req-build-8ec3wel5'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: 　https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch===1.13.1\n",
            "  Using cached torch-1.13.1-cp39-cp39-win_amd64.whl (162.5 MB)\n",
            "Collecting torchvision===0.14.1\n",
            "  Downloading torchvision-0.14.1-cp39-cp39-win_amd64.whl (1.1 MB)\n",
            "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
            "     -------------- ------------------------- 0.4/1.1 MB 12.9 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 0.8/1.1 MB 10.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 1.1/1.1 MB 8.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch===1.13.1) (4.4.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torchvision===0.14.1) (1.23.5)\n",
            "Requirement already satisfied: requests in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torchvision===0.14.1) (2.28.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torchvision===0.14.1) (9.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->torchvision===0.14.1) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->torchvision===0.14.1) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->torchvision===0.14.1) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shimp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->torchvision===0.14.1) (1.26.12)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0\n",
            "    Uninstalling torch-1.13.0:\n",
            "      Successfully uninstalled torch-1.13.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Location '　https://download.pytorch.org/whl/torch_stable.html' is ignored: it is either a non-existing path or lacks a specific scheme.\n",
            "WARNING: Location '　https://download.pytorch.org/whl/torch_stable.html' is ignored: it is either a non-existing path or lacks a specific scheme.\n",
            "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\shimp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR: Could not install packages due to an OSError: [WinError 5] アクセスが拒否されました。: 'C:\\\\Users\\\\shimp\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python39\\\\site-packages\\\\~orch\\\\lib\\\\asmjit.dll'\n",
            "Check the permissions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python3.9.exe -m pip install pip --upgrade\n",
        "!python3.9.exe -m pip install git+https://github.com/openai/whisper.git\n",
        "!python3.9.exe -m pip install torch===1.13.1 torchvision===0.14.1 -f　https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# %python3.11 -m pip install whisper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrHo70KLHlNc",
        "outputId": "2a898bff-8d24-4047-a793-11c0857a7e9b"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "from whisper.transcribe import format_timestamp\n",
        "from typing import Tuple, List\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m whisper\u001b[39m.\u001b[39mload_model(\u001b[39m\"\u001b[39m\u001b[39mlarge\u001b[39m\u001b[39m\"\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mhalf()\n\u001b[1;32m----> 4\u001b[0m model\u001b[39m.\u001b[39;49mcuda()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:747\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    731\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \n\u001b[0;32m    733\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 747\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:662\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 662\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    663\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    664\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:747\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    731\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \n\u001b[0;32m    733\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 747\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\cuda\\__init__.py:221\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    218\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    220\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 221\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    222\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    224\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "\n",
        "model = whisper.load_model(\"large\", device='cpu')\n",
        "_ = model.half()\n",
        "\n",
        "# model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "00:00:01,000 --> 00:00:02,000\n",
            "he\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def jimaku_srt(start_end_texts: List[Tuple[float, float, str]]) -> str:\n",
        "  res: List[str] = []\n",
        "  for i, (start,end,text) in enumerate(start_end_texts):\n",
        "    start_str = format_timestamp(start, always_include_hours=True, decimal_marker=',')\n",
        "    end_str = format_timestamp(end, always_include_hours=True, decimal_marker=',')\n",
        "    res.append(f\"{i+1}\")\n",
        "    res.append(f\"{start_str} --> {end_str}\")\n",
        "    res.append(f\"{text}\")\n",
        "    res.append(\"\")\n",
        "  return '\\n'.join(res)\n",
        "\n",
        "\n",
        "print(jimaku_srt([(1, 2, \"he\")]))\n",
        "\n",
        "\n",
        "# sample\n",
        "# =========\n",
        "# 1\n",
        "# 00:00:05,000 --> 00:00:10,000\n",
        "# これは\n",
        "# \n",
        "# 2\n",
        "# 00:00:10,000 --> 00:00:15,000\n",
        "# ｋ本的に\n",
        "# （きほんてきに）\n",
        "# \n",
        "# 3\n",
        "# 00:00:15,000 --> 00:00:20,000\n",
        "# テストです。\n",
        "# \n",
        "# 4\n",
        "# 00:00:20,000 --> 00:00:25,000\n",
        "# こんにちわ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def jimaku_webvtt(title: str, start_end_texts: List[Tuple[float, float, str]]) -> str:\n",
        "  res: List[str] = []\n",
        "  res.append(f\"WEBVTT - {title}\")\n",
        "  for i, (start,end,text) in enumerate(start_end_texts):\n",
        "    start_str = format_timestamp(start, always_include_hours=True, decimal_marker='.')\n",
        "    end_str = format_timestamp(end, always_include_hours=True, decimal_marker='.')\n",
        "    res.append(f\"{i+1}\")\n",
        "    res.append(f\"{start_str} --> {end_str}\")\n",
        "    res.append(f\"{text}\")\n",
        "    res.append(\"\")\n",
        "  return '\\n'.join(res)\n",
        "\n",
        "\n",
        "# sample\n",
        "# =======\n",
        "# WEBVTT - 好きな映画の翻訳\n",
        "# \n",
        "# NOTE\n",
        "# 何人かの友人が彼らの両親と一緒にそれを見ることができるように、\n",
        "# この翻訳は Kyle によってされました。\n",
        "# \n",
        "# 1\n",
        "# 00:02:15.000 --> 00:02:20.000\n",
        "# - Ta en kopp varmt te.\n",
        "# - Det är inte varmt.\n",
        "# \n",
        "# 2\n",
        "# 00:02:20.000 --> 00:02:25.000\n",
        "# - Har en kopp te.\n",
        "# - Det smakar som te.\n",
        "# \n",
        "# NOTE この最後の行はうまく翻訳されていないかもしれません。\n",
        "# \n",
        "# 3\n",
        "# 00:02:25.000 --> 00:02:30.000\n",
        "# - Ta en kopp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:00:01.000]he\n"
          ]
        }
      ],
      "source": [
        "def kashi_lrc(start_texts: List[Tuple[float, str]]) -> str:\n",
        "  res: List[str] = []\n",
        "  for (start,text) in start_texts:\n",
        "    start_str = format_timestamp(start, always_include_hours=True, decimal_marker='.')\n",
        "    res.append(f\"[{start_str}]{text}\")\n",
        "  return '\\n'.join(res)\n",
        "# Sapmle\n",
        "# ========\n",
        "# [00:05.00]ありがとう\n",
        "# [00:13.00] \n",
        "# [00:19.00]はじめまして\n",
        "\n",
        "print(kashi_lrc([(1, \"he\")]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['E:\\\\文字起こし待ち\\\\ダメ人間会_2023-01-14 23-11-33.mkv', 'E:\\\\文字起こし待ち\\\\ダメ人間会_2023-01-20 23-11-30.mkv', 'E:\\\\文字起こし待ち\\\\ダメ人間会_2023-01-21 23-23-21.mkv', 'E:\\\\文字起こし待ち\\\\ダメ人間会_2023-01-27 22-41-25.mkv', 'E:\\\\文字起こし待ち\\\\ダメ人間会_2023-02-03 22-09-15.mkv', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20180920_無修正 ライブチャット 素人 美少女 オナニー アイドル 可愛い 配信 チャ バイブ.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20181107_Mat438_1.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20190215_15232137.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20190331_rok859_rok (5).mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20200317_cha817_ch03.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20200322_cha817_ch04_ch04 (4).mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20201025_ari281_ari281_ar08_600609262.564195.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20220412_上戸彩\\u3000胸揉まれる.wmv', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20220723_nin178_ni02.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20180428_livekupa☆☆.7z\\\\【無修正\\u3000ライブチャット】\\u3000めっちゃかわいい貧乳で童顔のお姉さんがカメラの前でお○んこをクパァと披露して、指でクチュクチュオナニーしちゃいます♪.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20180718_shiolive\\\\44625840.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20180719_meido_solt\\\\【無修正】メイドコスプレでびしょびしょ大量潮吹き美女ライブチャット.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20180922_mu_live_kupa_akum\\\\無修正 ライブチャット 個人撮影 美少女 オナニー パイパン ギャル 個人撮 くぱぁ チャット動画.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20181026_lc - TOKYO Motion (1)\\\\lc - TOKYO Motion (1).mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20220818_cho292_ch04_e\\\\1.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20220818_cho292_ch04_e\\\\2.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20220818_cho292_ch04_e\\\\3.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20220925_oyu148_o2\\\\お礼に指マン.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20220925_oyu148_o2\\\\フェラ.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20221030_red_mizutama\\\\無修正 ライブチャット パイパン オナニー チャット動画 自撮り 制服 可愛い ディルド 美女.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20221030_red_mizutama\\\\無修正 ライブチャット 個人撮影 アナル オナニー 個人撮 清楚 ディルド ディルドオナニー チャ.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20221030_red_mizutama\\\\無修正 ライブチャット 個人撮影 美少女 パイパン 貧乳 配信 個人撮 パンツ ツインテール.mp4', 'E:\\\\onlinestrage\\\\import_Evernote\\\\Whisper\\\\20221031_iomdghsdffff\\\\iomdghsdffff - Pornhub.com.mp4', 'H:\\\\LoiLo Game\\\\スマホ動画無圧縮\\\\20190719\\\\QVR_2019_07_19_10_01_43.mp9', 'H:\\\\LoiLo Game\\\\スマホ動画無圧縮\\\\20190719\\\\QVR_2019_07_19_10_28_53.mp9', 'H:\\\\LoiLo Game\\\\スマホ動画無圧縮\\\\20190719\\\\QVR_2019_07_19_10_29_11.mp9', 'H:\\\\LoiLo Game\\\\スマホ動画無圧縮\\\\20190719\\\\QVR_2019_07_19_10_29_37.mp9', 'H:\\\\LoiLo Game\\\\スマホ動画無圧縮\\\\20190719\\\\QVR_2019_07_19_10_31_19.mp9', 'H:\\\\LoiLo Game\\\\スマホ動画無圧縮\\\\20190719\\\\QVR_2019_07_19_11_01_20.mp9', 'H:\\\\LoiLo Game\\\\スマホ動画無圧縮\\\\20190719\\\\QVR_2019_07_19_11_31_21.mp9']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def collect_audio(dir_pathes: str) -> List[str]:\n",
        "  exts = [\".wav\", \".mp3\", \".ogg\", \".mp4\", \".mkv\", \".avi\", \".acc\", \".m4a\", \".opus\", \".wmv\", \".wma\", \".mp9\", \".webm\"]\n",
        "  results = []\n",
        "  for dir_path in dir_pathes:\n",
        "    for root, _, files in os.walk(dir_path):\n",
        "      for file in files:\n",
        "        ext = os.path.splitext(file)[1]\n",
        "        if ext in exts:\n",
        "          results.append(os.path.join(root,file))\n",
        "  return results\n",
        "\n",
        "\n",
        "dir_pathes = [\n",
        "  r\"E:\\文字起こし待ち\",\n",
        "  r\"C:\\Users\\shimp\\Videos\\game\",\n",
        "  r\"E:\\onlinestrage\\import_Evernote\\Whisper\",\n",
        "  r\"E:\\onlinestrage\\import_Evernote\\アップロード待機\\20221119_hat202\",\n",
        "  r\"H:\\LoiLo Game\\スマホ動画無圧縮\\20190719\",\n",
        "  # r\"E:\\onlinestrage\\import_Evernote\\アップロード待機\",\n",
        "]\n",
        "audio_pathes = collect_audio(dir_pathes)\n",
        "print(audio_pathes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2xZYoMRH7If",
        "outputId": "e9732fa7-4776-4ace-b775-9499d1d9bcff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/35] E:\\文字起こし待ち\\ダメ人間会_2023-01-14 23-11-33.mkv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1148793 [00:00<?, ?frames/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "expected scalar type Float but found Half",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m lrc_filepath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dirpath, lrc_filename)\n\u001b[0;32m     26\u001b[0m lang \u001b[39m=\u001b[39m get_language(audio_path)\n\u001b[1;32m---> 28\u001b[0m result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtranscribe(audio_path, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, fp16\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, language\u001b[39m=\u001b[39;49mlang)\n\u001b[0;32m     29\u001b[0m results\u001b[39m.\u001b[39mappend((audio_path, result))\n\u001b[0;32m     30\u001b[0m \u001b[39m# print(result[\"text\"])\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\whisper\\transcribe.py:183\u001b[0m, in \u001b[0;36mtranscribe\u001b[1;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, **decode_options)\u001b[0m\n\u001b[0;32m    180\u001b[0m segment_duration \u001b[39m=\u001b[39m segment\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m HOP_LENGTH \u001b[39m/\u001b[39m SAMPLE_RATE\n\u001b[0;32m    182\u001b[0m decode_options[\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m all_tokens[prompt_reset_since:]\n\u001b[1;32m--> 183\u001b[0m result: DecodingResult \u001b[39m=\u001b[39m decode_with_fallback(segment)\n\u001b[0;32m    184\u001b[0m tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(result\u001b[39m.\u001b[39mtokens)\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m no_speech_threshold \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[39m# no voice activity check\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\whisper\\transcribe.py:118\u001b[0m, in \u001b[0;36mtranscribe.<locals>.decode_with_fallback\u001b[1;34m(segment)\u001b[0m\n\u001b[0;32m    115\u001b[0m     kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mbest_of\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    117\u001b[0m options \u001b[39m=\u001b[39m DecodingOptions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs, temperature\u001b[39m=\u001b[39mt)\n\u001b[1;32m--> 118\u001b[0m decode_result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdecode(segment, options)\n\u001b[0;32m    120\u001b[0m needs_fallback \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39mif\u001b[39;00m compression_ratio_threshold \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decode_result\u001b[39m.\u001b[39mcompression_ratio \u001b[39m>\u001b[39m compression_ratio_threshold:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\whisper\\decoding.py:707\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(model, mel, options)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mif\u001b[39;00m single \u001b[39m:=\u001b[39m mel\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    705\u001b[0m     mel \u001b[39m=\u001b[39m mel\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 707\u001b[0m result \u001b[39m=\u001b[39m DecodingTask(model, options)\u001b[39m.\u001b[39;49mrun(mel)\n\u001b[0;32m    709\u001b[0m \u001b[39mreturn\u001b[39;00m result[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m single \u001b[39melse\u001b[39;00m result\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\whisper\\decoding.py:624\u001b[0m, in \u001b[0;36mDecodingTask.run\u001b[1;34m(self, mel)\u001b[0m\n\u001b[0;32m    621\u001b[0m tokenizer: Tokenizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\n\u001b[0;32m    622\u001b[0m n_audio: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m mel\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 624\u001b[0m audio_features: Tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_audio_features(mel)  \u001b[39m# encoder forward pass\u001b[39;00m\n\u001b[0;32m    625\u001b[0m tokens: Tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial_tokens])\u001b[39m.\u001b[39mrepeat(n_audio, \u001b[39m1\u001b[39m)\n\u001b[0;32m    627\u001b[0m \u001b[39m# detect language if requested, overwriting the language token\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\whisper\\decoding.py:568\u001b[0m, in \u001b[0;36mDecodingTask._get_audio_features\u001b[1;34m(self, mel)\u001b[0m\n\u001b[0;32m    566\u001b[0m     audio_features \u001b[39m=\u001b[39m mel\n\u001b[0;32m    567\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 568\u001b[0m     audio_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mencoder(mel)\n\u001b[0;32m    570\u001b[0m \u001b[39mif\u001b[39;00m audio_features\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m (torch\u001b[39m.\u001b[39mfloat16 \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mfp16 \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mfloat32):\n\u001b[0;32m    571\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maudio_features has an incorrect dtype: \u001b[39m\u001b[39m{\u001b[39;00maudio_features\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m def _call_impl(self, *input, **kwargs):\n\u001b[0;32m   1189\u001b[0m     forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)\n\u001b[1;32m-> 1190\u001b[0m     # If we don't have any hooks, we want to skip the rest of the logic in\n\u001b[0;32m   1191\u001b[0m     # this function, and just call forward.\n\u001b[0;32m   1192\u001b[0m     if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m             or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m   1194\u001b[0m         return forward_call(*input, **kwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\whisper\\model.py:157\u001b[0m, in \u001b[0;36mAudioEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    154\u001b[0m x \u001b[39m=\u001b[39m (x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    156\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[1;32m--> 157\u001b[0m     x \u001b[39m=\u001b[39m block(x)\n\u001b[0;32m    159\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_post(x)\n\u001b[0;32m    160\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m def _call_impl(self, *input, **kwargs):\n\u001b[0;32m   1189\u001b[0m     forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)\n\u001b[1;32m-> 1190\u001b[0m     # If we don't have any hooks, we want to skip the rest of the logic in\n\u001b[0;32m   1191\u001b[0m     # this function, and just call forward.\n\u001b[0;32m   1192\u001b[0m     if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m             or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m   1194\u001b[0m         return forward_call(*input, **kwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\whisper\\model.py:125\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[1;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    119\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    120\u001b[0m     x: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m     kv_cache: Optional[\u001b[39mdict\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    124\u001b[0m ):\n\u001b[1;32m--> 125\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn_ln(x), mask\u001b[39m=\u001b[39mmask, kv_cache\u001b[39m=\u001b[39mkv_cache)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    126\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcross_attn:\n\u001b[0;32m    127\u001b[0m         x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcross_attn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcross_attn_ln(x), xa, kv_cache\u001b[39m=\u001b[39mkv_cache)[\u001b[39m0\u001b[39m]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m def _call_impl(self, *input, **kwargs):\n\u001b[0;32m   1189\u001b[0m     forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)\n\u001b[1;32m-> 1190\u001b[0m     # If we don't have any hooks, we want to skip the rest of the logic in\n\u001b[0;32m   1191\u001b[0m     # this function, and just call forward.\n\u001b[0;32m   1192\u001b[0m     if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m             or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m   1194\u001b[0m         return forward_call(*input, **kwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\whisper\\model.py:31\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(x\u001b[39m.\u001b[39;49mfloat())\u001b[39m.\u001b[39mtype(x\u001b[39m.\u001b[39mdtype)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[0;32m    191\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\functional.py:2515\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2511\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[0;32m   2512\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2513\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[0;32m   2514\u001b[0m     )\n\u001b[1;32m-> 2515\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Float but found Half"
          ]
        }
      ],
      "source": [
        "\n",
        "def get_language(audio_path: str) -> str:\n",
        "  if 'LianaGood' in audio_path:\n",
        "    return 'en'\n",
        "  else:\n",
        "    return 'ja'\n",
        "\n",
        "results = []\n",
        "for i, audio_path in enumerate(audio_pathes):\n",
        "  print(f\"[{i+1}/{len(audio_pathes)}] {audio_path}\")\n",
        "  \n",
        "  filebasename = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "  dirpath = os.path.dirname(audio_path)\n",
        "  all_filename = filebasename + '_whisper.json'\n",
        "  all_filepath = os.path.join(dirpath, all_filename)\n",
        "  if os.path.exists(all_filepath):\n",
        "    continue\n",
        "  text_filename = filebasename + '_whisper.txt'\n",
        "  text_filepath = os.path.join(dirpath, text_filename)\n",
        "  srt_filename = filebasename + '_whisper.srt'\n",
        "  srt_filepath = os.path.join(dirpath, srt_filename)\n",
        "  webvtt_filename = filebasename + '_whisper.vtt'\n",
        "  webvtt_filepath = os.path.join(dirpath, webvtt_filename)\n",
        "  lrc_filename = filebasename + '.lrc'\n",
        "  lrc_filepath = os.path.join(dirpath, lrc_filename)\n",
        "\n",
        "  lang = get_language(audio_path)\n",
        "\n",
        "  result = model.transcribe(audio_path, verbose=False, fp16=False, language=lang)\n",
        "  results.append((audio_path, result))\n",
        "  # print(result[\"text\"])\n",
        "\n",
        "  with open(all_filepath, 'w', encoding='utf-8') as fp:\n",
        "    fp.writelines(json.dumps(result, ensure_ascii=False, indent=2))\n",
        "  with open(text_filepath, 'w', encoding='utf-8') as fp:\n",
        "    fp.writelines(result[\"text\"])\n",
        "  start_end_texts = [(x[\"start\"], x[\"end\"], x[\"text\"]) for x in result[\"segments\"]]\n",
        "  with open(srt_filepath, 'w', encoding='utf-8') as fp:\n",
        "    fp.writelines(jimaku_srt(start_end_texts))\n",
        "  with open(webvtt_filepath, 'w', encoding='utf-8') as fp:\n",
        "    fp.writelines(jimaku_webvtt(filebasename, start_end_texts))\n",
        "  with open(lrc_filepath, 'w', encoding='utf-8') as fp:\n",
        "    fp.writelines(kashi_lrc([(x[0], x[2]) for x in start_end_texts]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E:\\onlinestrage\\import_Evernote\\アップロード待機\\20221119_hat202\\hat2023.mp4\n",
            "H:\\LoiLo Game\\スマホ動画無圧縮\\20190719\\QVR_2019_07_19_10_01_43.mp9\n"
          ]
        }
      ],
      "source": [
        "for audio_path, result in results:\n",
        "  print(audio_path)\n",
        "  filebasename = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "  dirpath = os.path.dirname(audio_path)\n",
        "  all_filename = filebasename + '_whisper.json'\n",
        "  all_filepath = os.path.join(dirpath, all_filename)\n",
        "  text_filename = filebasename + '_whisper.txt'\n",
        "  text_filepath = os.path.join(dirpath, text_filename)\n",
        "  srt_filename = filebasename + '_whisper.srt'\n",
        "  srt_filepath = os.path.join(dirpath, srt_filename)\n",
        "  webvtt_filename = filebasename + '_whisper.vtt'\n",
        "  webvtt_filepath = os.path.join(dirpath, webvtt_filename)\n",
        "  lrc_filename = filebasename + '.lrc'\n",
        "  lrc_filepath = os.path.join(dirpath, lrc_filename)\n",
        "\n",
        "  with open(all_filepath, 'w', encoding='utf-8') as fp:\n",
        "    fp.writelines(json.dumps(result, ensure_ascii=False, indent=2))\n",
        "  with open(text_filepath, 'w', encoding='utf-8') as fp:\n",
        "    fp.writelines(result[\"text\"])\n",
        "  start_end_texts = [(x[\"start\"], x[\"end\"], x[\"text\"]) for x in result[\"segments\"]]\n",
        "  with open(srt_filepath, 'w', encoding='utf-8') as fp:\n",
        "    fp.writelines(jimaku_srt(start_end_texts))\n",
        "  with open(webvtt_filepath, 'w', encoding='utf-8') as fp:\n",
        "    fp.writelines(jimaku_webvtt(filebasename, start_end_texts))\n",
        "  with open(lrc_filepath, 'w', encoding='utf-8') as fp:\n",
        "    fp.writelines(kashi_lrc([(x[0], x[2]) for x in start_end_texts]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kE9UMh6JA4M"
      },
      "source": [
        "# 新しいセクション"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5d-qMJ1IBs8"
      },
      "outputs": [],
      "source": [
        "result = model.transcribe(\"audio_sample.wav\", verbose=True, language=\"ja\", task=\"translate\")\n",
        "print(result[\"text\"])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "4baf6ddc1deb9fbbc0307abc0903d090887e21379b698081220ddbb38ece70d4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
